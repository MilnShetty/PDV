{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class AmazonBatSpider(scrapy.Spider):\n",
    "    name = 'amazon_bat'\n",
    "    start_urls = ['https://www.amazon.in/s?k=bat&crid=1B5LH49P1VN8M&sprefix=bat%2Caps%2C255&ref=nb_sb_noss_1']\n",
    "    page_count = 0  # Initialize a page count\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extract data from the current page\n",
    "        product_listings = response.xpath('//div[@data-asin]')\n",
    "\n",
    "        for product in product_listings:\n",
    "            item = {\n",
    "                'name': product.xpath('.//span[@class=\"a-size-base-plus a-color-base a-text-normal\"]/text()').get(),\n",
    "                'price': product.xpath('.//span[@class=\"a-price-whole\"]/text()').get(),\n",
    "                'rating': product.xpath('.//span[@class=\"a-icon-alt\"]/text()').get(),\n",
    "            }\n",
    "            yield item\n",
    "\n",
    "        # Increment the page count\n",
    "        self.page_count += 1\n",
    "\n",
    "        # Check if we have scraped 5 pages, and if not, continue to the next page\n",
    "        if self.page_count < 5:\n",
    "            next_page = response.css('a.s-pagination-next::attr(href)').get()\n",
    "            if next_page:\n",
    "                next_page_url = response.urljoin(next_page)\n",
    "                yield scrapy.Request(next_page_url, callback=self.parse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "item.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class AmazonPhonesItem(scrapy.Item):\n",
    "    name = scrapy.Field()\n",
    "    price = scrapy.Field()\n",
    "    rating = scrapy.Field()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to start project :\n",
    "1. create one folder \n",
    "2. open anaconda powershell as run as adminstrtor\n",
    "\t1.set location path:>> cd C:\\Users\\MILAN\\OneDrive\\Desktop\\MilanPDV_AIML2023\n",
    "\t2.>> scrapy startproject amazon\n",
    "\t3. two new folder will be created of amazon \n",
    "3.open spyder open the newly created folder  and start coding\n",
    "4.How to run the project :\n",
    "\t1.>> C:\\Users\\MILAN\\OneDrive\\Desktop\\MilanPDV_AIML2023\\amazon\\amazon\n",
    "\t2.>> cd .\\spiders\\\n",
    "\t3.you will get >> C:\\Users\\MILAN\\OneDrive\\Desktop\\MilanPDV_AIML2023\\amazon\\amazon\\spiders\n",
    "\t4.after setting the path you are ready to run\n",
    "\t5.>> scrapy crawl amazon_bat  (i gave name as amazon_bat)\n",
    "5.You can export the scraped data to a CSV, JSON, or other formats\n",
    "\t1.>> scrapy crawl amazon_bat -o output.csv\n",
    "    2.>> scrapy crawl phones -o output.json\n",
    "\n",
    "\n",
    "Debugging with Scrapy Shell:\n",
    "\n",
    "Use the Scrapy shell to interactively test your selectors and XPath expressions. \n",
    "You can run the shell with a specific URL and try selecting elements to see if they are correctly targeted.\n",
    "Here's how you can use the Scrapy shell for debugging:\n",
    ">> scrapy shell 'https://quotes.toscrape.com/page/1/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
